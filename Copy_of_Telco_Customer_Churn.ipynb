{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "PoPl-ycgm1ru",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "yiiVWRdJDDil",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "TIqpNgepFxVj",
        "ArJBuiUVfxKd",
        "TfvqoZmBfxKf"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohan1-tech/Telco-Customer-Churn/blob/main/Copy_of_Telco_Customer_Churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  **Telco Customer Churn**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -Telco Customer Churn**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Summary of the Telco Customer Churn Dataset**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The Telco Customer Churn dataset contains information about a telecommunications company's customers and whether they have discontinued service (churned). It comprises 7043 records and 21 columns representing various customer attributes, services, and billing details. The dataset is widely used for churn prediction modeling to understand factors influencing customer retention and attrition.\n",
        "\n",
        "\n",
        "#**Dataset Structure and Features**\n",
        "\n",
        "\n",
        "\n",
        "1.   **CustomerID**: Unique identifier for each customer.\n",
        "2.   **Demographic Features**:\n",
        "\n",
        "#**Demographic Features**:\n",
        "\n",
        "**gender**: Customer's gender (Male/Female).\n",
        "\n",
        "**SeniorCitizen**: Binary indicator (0 = No, 1 = Yes) if the customer is a senior citizen.\n",
        "\n",
        "**Partner**: Whether the customer has a partner (Yes/No).\n",
        "\n",
        "**Dependents**: Whether the customer has dependents (Yes/No).\n",
        "\n",
        "**Account and Service Details**:\n",
        "\n",
        "**tenure**: Number of months the customer has stayed with the company.\n",
        "\n",
        "**PhoneService**: Whether the customer has phone service (Yes/No).\n",
        "\n",
        "**MultipleLines**: Whether the customer has multiple phone lines (Yes/No/No phone service).\n",
        "\n",
        "**InternetService**: Type of internet service (DSL, Fiber optic, No).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#**Data Characteristics and Observations**\n",
        "\n",
        "* **Balanced Features**: The dataset has a mix of categorical, numerical, and binary features covering customer demographics, services, and financials, making it comprehensive for churn analysis.\n",
        "\n",
        "* **Tenure and Churn**: The tenure feature is crucial as it measures customer loyalty duration. Customers with short tenure are typically at higher risk of churn.\n",
        "\n",
        "\n",
        "* **Services Impact**: Features such as InternetService type and value-added services (e.g., OnlineSecurity, TechSupport) may significantly influence customer satisfaction and churn behavior.\n",
        "\n",
        "* **Contract Type Influence**: Contract terms likely impact churn rates; month-to-month contracts generally have higher churn compared to annual or biennial contracts.\n",
        "\n",
        "* **Contract Type Influence**: Contract terms likely impact churn rates; month-to-month contracts generally have higher churn compared to annual or biennial contracts.\n",
        "\n",
        "* **Billing Method**: Paperless billing and payment methods could correlate with churn patterns, especially electronic check payments, which sometimes show higher churn.\n",
        "\n",
        "* **Data Quality Note**: The TotalCharges column is stored as an object type, suggesting some non-numeric entries or formatting issues, which will require cleaning before analysis.\n",
        "\n",
        "\n",
        "\n",
        "#**Potential Use Cases of the Dataset**\n",
        "\n",
        "* **Churn Prediction Models**: Using the dataset to build machine learning models that predict whether a customer will churn, enabling targeted retention efforts.\n",
        "\n",
        "* **Customer Segmentation**: Segment customers by demographics, tenure, and service usage to understand different profiles and design personalized offers.\n",
        "\n",
        "* **Feature Importance Analysis**: Determine which factors (services, contract type, demographics) are the strongest predictors of churn.\n",
        "\n",
        "* **Business Insights**: Help the company understand the impact of various service offerings and billing options on customer loyalty.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The telecommunications industry faces significant challenges in retaining customers due to intense competition and the ease of switching service providers. Customer churn—when a customer stops doing business with the company—directly impacts revenue and profitability.\n",
        "\n",
        "The objective is to analyze historical customer data from Telco to identify the key factors contributing to churn and to **build a predictive model** that can accurately determine the likelihood of a customer leaving. By understanding churn drivers and predicting at-risk customers, the company can implement targeted retention strategies, reduce churn rates, and improve customer satisfaction.\n",
        "\n",
        "\n",
        "**Key Goals**:\n",
        "\n",
        "**Understand churn patterns** – Identify which customer segments and service attributes are strongly linked to higher churn rates.\n",
        "\n",
        "**Develop a churn prediction model** – Use machine learning techniques to predict whether a customer will churn based on historical data.\n",
        "\n",
        "**Enable data-driven retention strategies** – Provide insights for customer service and marketing teams to proactively retain customers"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions category_encoders transformers\n"
      ],
      "metadata": {
        "id": "7aBI4KAc3wDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.mosaicplot import mosaic\n",
        "import seaborn as sns\n",
        "import contractions\n",
        "import string\n",
        "import category_encoders as ce\n",
        "import re\n",
        "import nltk\n",
        "import contractions\n",
        "import contractions\n",
        "import category_encoders as ce\n",
        "import nltk\n",
        "\n",
        "\n",
        "# Machine learning and model evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, word_tokenize\n",
        "# Install all required libraries in one go\n",
        "\n",
        "\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "from collections import OrderedDict\n",
        "from scipy.stats import ttest_ind\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import contractions\n",
        "import string\n",
        "import category_encoders as ce\n",
        "from statsmodels.graphics.mosaicplot import mosaic\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "# To ignore warnings (optional)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Data/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "print(\"Number of rows:\", df.shape[0])\n",
        "print(\"Number of columns:\", df.shape[1])\n",
        "\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "\n",
        "print(\"Dataset Information :\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "print(\"Duplicate Values :\")\n",
        "print(df.duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"Missing Values :\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "# Create a heatmap of missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cmap='viridis', cbar=False)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Dataset Details**\n",
        "\n",
        "\n",
        "\n",
        "* It contains customer information for a telecom company.\n",
        "\n",
        "* Each row represents one unique customer (no duplicates).\n",
        "\n",
        "* The target column is Churn — whether the customer left (Yes) or stayed (No).\n",
        "\n",
        "\n",
        "\n",
        "**Main features include**:\n",
        "\n",
        "\n",
        "* Demographics: gender, SeniorCitizen, Partner, Dependents\n",
        "\n",
        "* Services used: PhoneService, InternetService, OnlineSecurity, TechSupport, StreamingTV, etc.\n",
        "\n",
        "* Account info: tenure (months with company), Contract type, PaymentMethod, PaperlessBilling\n",
        "\n",
        "* Charges: MonthlyCharges, TotalCharges\n",
        "\n",
        "**2. Data Quality**\n",
        "\n",
        "* No missing values\n",
        "\n",
        "* No duplicate rows\n",
        "\n",
        "* All records have valid values.\n",
        "\n",
        "\n",
        "**3. Problem to Solve**\n",
        "\n",
        "\n",
        "* Goal: Understand why customers churn and predict which ones are likely to churn.\n",
        "\n",
        "* This will help the company take action to retain them."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert to numeric and handle errors**"
      ],
      "metadata": {
        "id": "zhm5_1y4OUJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to numeric and handle errors\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "df['MonthlyCharges'] = pd.to_numeric(df['MonthlyCharges'], errors='coerce')\n",
        "\n",
        "# Handle missing values\n",
        "df[['TotalCharges', 'MonthlyCharges']] = df[['TotalCharges', 'MonthlyCharges']].fillna(df[['TotalCharges', 'MonthlyCharges']].mean())\n",
        "# OR drop rows with missing\n",
        "# df = df.dropna(subset=['TotalCharges', 'MonthlyCharges'])\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "9AdROVyEOJ3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"Dataset Columns :\")\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "print(\"Dataset Describe :\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here is a clear description of the variables (columns) in your Telco Customer Churn dataset**:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**customerID**: Unique identifier for each customer.\n",
        "\n",
        "**gender**: The gender of the customer (Male or Female).\n",
        "\n",
        "**SeniorCitizen**: Binary indicator if the customer is a senior citizen (1 = Yes, 0 = No).\n",
        "\n",
        "**Partner**: Whether the customer has a partner (Yes or No).\n",
        "\n",
        "**Dependents**: Whether the customer has dependents (Yes or No).\n",
        "\n",
        "**tenure**: Number of months the customer has stayed with the company.\n",
        "\n",
        "**PhoneService**: Whether the customer has phone service (Yes or No).\n",
        "\n",
        "**MultipleLines**: Whether the customer has multiple phone lines (Yes, No, or No phone service).\n",
        "\n",
        "**InternetService**: Customer’s internet service provider type (DSL, Fiber optic, or No internet).\n",
        "\n",
        "**OnlineSecurity**: Whether the customer has online security add-on (Yes, No, or No internet service).\n",
        "\n",
        "**OnlineBackup**: Whether the customer has online backup add-on (Yes, No, or No internet service).\n",
        "\n",
        "**DeviceProtection**: Whether the customer has device protection add-on (Yes, No, or No internet service).\n",
        "\n",
        "**TechSupport**: Whether the customer has tech support add-on (Yes, No, or No internet service).\n",
        "\n",
        "**StreamingTV**: Whether the customer has streaming TV service (Yes, No, or No internet service).\n",
        "\n",
        "**StreamingMovies**: Whether the customer has streaming movies service (Yes, No, or No internet service).\n",
        "\n",
        "**Contract**: Type of contract the customer has (Month-to-month, One year, or Two year).\n",
        "\n",
        "**PaperlessBilling**: Whether the customer uses paperless billing (Yes or No).\n",
        "\n",
        "**PaymentMethod**: Payment method used by the customer (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)).\n",
        "\n",
        "**MonthlyCharges**: The amount charged monthly to the customer.\n",
        "\n",
        "**TotalCharges**: The total amount charged to the customer over the tenure.\n",
        "\n",
        "**Churn**: Target variable—whether the customer has churned (Yes or No).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**This description helps understand the role of each variable for your analysis and predictive modeling of customer churn. If you want, I can also help you explore or visualize these variables to gain more insights**."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "print(\"Unique Values for each variable :\")\n",
        "for column in df.columns:\n",
        "    np.unique_values = df[column].unique()\n",
        "    print(f\"{column}: {np.unique_values}\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Count of Unique Values for each variable:\")\n",
        "print(df.nunique())\n"
      ],
      "metadata": {
        "id": "_dmhinMJfSUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Convert columns with numeric data stored as strings to proper numeric type\n",
        "\n",
        "# Convert 'TotalCharges' to numeric, coercing errors to NaN\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "# Check for missing values that result from conversion\n",
        "print(\"Missing values in TotalCharges after conversion:\", df['TotalCharges'].isnull().sum())\n",
        "\n",
        "\n",
        "# Fill missing values in 'TotalCharges' with median (safe assumption)\n",
        "df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
        "\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    df[col] = df[col].str.strip()\n",
        "\n",
        "\n",
        "print(df.info())\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Pivot Table\n",
        "\n",
        "# Simple Pivot Table Example\n",
        "pivot = pd.pivot_table(\n",
        "    df,\n",
        "    values='MonthlyCharges',\n",
        "    index='Contract',\n",
        "    columns='Churn',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "print(pivot)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ahLUeLcEvhuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What this does**:\n",
        "\n",
        "Shows the average monthly charges for each contract type, split by whether the customer churned or not."
      ],
      "metadata": {
        "id": "JOL0FiOIvqYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pivot2 = pd.pivot_table(\n",
        "    df,\n",
        "    values='TotalCharges',\n",
        "    index=['InternetService', 'SeniorCitizen'],\n",
        "    columns='Churn',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "print(pivot2)\n"
      ],
      "metadata": {
        "id": "Mp_x7opPv2L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Use Pivot Tables?**\n",
        "\n",
        "* Summarize your data quickly.\n",
        "\n",
        "* Find patterns and compare categories (e.g., who pays more, who churns more)."
      ],
      "metadata": {
        "id": "dqUB-vCkv5OC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "**1.Converted TotalCharges to Numeric**:\n",
        "\n",
        "* Changed TotalCharges from string/object type to numeric (float64), coercing errors to NaN.\n",
        "\n",
        "\n",
        "\n",
        "**2.Handled Missing Values in TotalCharges**:\n",
        "\n",
        "* Filled NaN values (created by coercion) with the median value, ensuring no missing data remains.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**3.Cleaned String Columns**:\n",
        "\n",
        "* Stripped whitespace from all object/string columns to avoid inconsistencies during analysis and encoding.\n",
        "\n",
        "\n",
        "**4.Verified Data Types and Completeness**:\n",
        "\n",
        "* Confirmed no missing values exist and data types are appropriately set for further analysis.\n",
        "\n",
        "**5.Created Pivot Tables for Summary Insights**:\n",
        "\n",
        "* Generated pivot tables showing average MonthlyCharges by Contract type and Churn status.\n",
        "\n",
        "* Generated pivot tables showing average TotalCharges by InternetService, SeniorCitizen status, and Churn.\n",
        "\n",
        "\n",
        "**Insights from Pivot Tables**\n",
        "\n",
        "**Monthly Charges by Contract and Churn**:\n",
        "* Customers on shorter contracts (e.g., Month-to-month) tend to have higher average monthly charges, especially those who churned. This could indicate price sensitivity plays a role in customer churn.\n",
        "\n",
        "**Total Charges by Internet Service, Senior Citizen Status, and Churn**:\n",
        "* Senior citizens with certain types of Internet service show different spending patterns. Customers who churn tend to have lower total charges, possibly indicating less engagement or shorter tenure.\n",
        "\n"
      ],
      "metadata": {
        "id": "UB96gi1DyC5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "# Calculate churn count by contract type\n",
        "churn_counts = df.groupby(['Contract', 'Churn']).size().reset_index(name='Count')\n",
        "\n",
        "# Plot the churn counts using barplot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(data=churn_counts, x='Contract', y='Count', hue='Churn')\n",
        "\n",
        "plt.title('Customer Churn Count by Contract Type')\n",
        "plt.xlabel('Contract Type')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.legend(title='Churn')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I picked the bar chart because it clearly shows and compares how many customers churn or stay within each contract type, making it easy to spot patterns."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The chart shows that customers with**Month-to-month** contracts have a much higher churn rate compared to those with**One year or Two year** contracts. This suggests that shorter contracts are associated with more customer churn."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "**Yes**, the insights can help create a positive business impact.\n",
        "\n",
        "Knowing that customers on Month-to-month contracts churn more enables the company to design targeted retention strategies, such as offering incentives or longer-term contracts to these customers, reducing churn and increasing revenue.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Regarding negative growth**:\n",
        "If the company relies heavily on short-term contracts without addressing high churn, it may face revenue loss and customer instability, harming growth. This reliance on easily churned customer segments can lead to negative growth if not managed properly"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# Average MonthlyCharges by Churn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Churn', y='MonthlyCharges', data=df)\n",
        "plt.title('Average Monthly Charges by Churn Status')\n",
        "plt.xlabel('Churn')\n",
        "plt.ylabel('Average Monthly Charges')\n",
        "plt.show()\n",
        "\n",
        "# Average TotalCharges by Churn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Churn', y='TotalCharges', data=df)\n",
        "plt.title('Average Total Charges by Churn Status')\n",
        "plt.xlabel('Churn')\n",
        "plt.ylabel('Average Total Charges')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose bar charts because they provide a straightforward way to compare average spending (**MonthlyCharges and TotalCharges**) between groups like churned vs. retained customers. This helps identify if customers who churn tend to pay more or less, offering actionable insights for pricing or retention strategies. Bar charts are simple to interpret and highlight these differences clearly."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "\n",
        "The chart reveals that customers who churn generally have higher average **MonthlyCharges** but lower average **TotalCharges** compared to those who stay. This suggests that churners often pay more each month but have shorter tenures, leading to lower overall spending."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, these insights can create a positive business impact by helping the company identify that customers with higher monthly charges but lower total spending are more likely to churn. The company can target these customers with tailored offers, discounts, or improved service to increase their loyalty and tenure, thereby boosting revenue.\n",
        "\n",
        "***\n",
        "\n",
        "However, if these high-paying, short-tenure customers continue to churn without intervention, the company risks **negative growth** due to lost revenue from valuable customers who pay more monthly but leave early. This pattern indicates revenue leakage and highlights the need for proactive retention strategies to prevent decline."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# Count customers by Payment Method and Churn status\n",
        "payment_churn_counts = df.groupby(['PaymentMethod', 'Churn']).size().reset_index(name='Count')\n",
        "\n",
        "# Plot with bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=payment_churn_counts, x='PaymentMethod', y='Count', hue='Churn')\n",
        "\n",
        "plt.title('Customer Churn Count by Payment Method')\n",
        "plt.xlabel('Payment Method')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Churn')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "This will help you identify if certain payment methods are associated with higher churn rates.\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The chart shows that customers using Electronic check as their payment method have a notably higher churn rate compared to those using other methods like Mailed check, Bank transfer (**automatic**), or Credit card (**automatic**). This suggests that customers paying electronically by check may be more likely to leave, indicating a potential area to focus retention efforts"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "\n",
        "Yes, the insights can help create a positive business impact by highlighting that customers using **Electronic check** payments churn more. The company can develop targeted retention strategies for this group, such as improving payment experience or offering incentives, to reduce churn and improve revenue stability.\n",
        "\n",
        "***\n",
        "\n",
        "However, if the high churn among **Electronic check** users is not addressed, it could lead to **negative growth** due to increased customer loss in this sizable payment segment, resulting in reduced recurring revenue and higher customer acquisition costs to replace lost customers."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "# Count customers by StreamingMovies and Churn\n",
        "streaming_churn_counts = df.groupby(['StreamingMovies', 'Churn']).size().reset_index(name='Count')\n",
        "\n",
        "# Plot the churn counts using barplot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(data=streaming_churn_counts, x='StreamingMovies', y='Count', hue='Churn', palette={'Yes':'blue', 'No':'orange'})\n",
        "\n",
        "plt.title('Customer Churn Count by Streaming Movies Subscription')\n",
        "plt.xlabel('Streaming Movies')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.legend(title='Churn')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "picked this bar chart because it clearly shows the relationship between streaming movie subscription and churn by comparing the number of customers who churned or stayed within each streaming category (**Yes or No**). It’s easy to understand and reveals important patterns in customer behavior related to service usage."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The chart shows that customers **not subscribing to Streaming Movies** have a higher churn count than those who do subscribe. This suggests that offering streaming movie services may help improve customer retention."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "\n",
        "Yes, the insights can help create a positive business impact by showing that customers who subscribe to streaming movies are less likely to churn. This suggests that promoting or bundling streaming services could improve retention and increase customer lifetime value.\n",
        "\n",
        "***\n",
        "\n",
        "However, if the company fails to offer or properly market streaming movie services, it risks **negative growth** from losing customers who seek entertainment options elsewhere, leading to higher churn and reduced revenue."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "# Create tenure groups (bins)\n",
        "bins = [0, 12, 24, 36, 48, 60, 72]\n",
        "labels = ['0-12', '13-24', '25-36', '37-48', '49-60', '61-72']\n",
        "df['TenureGroup'] = pd.cut(df['tenure'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "# Calculate churn rate by tenure group\n",
        "churn_rate = df.groupby('TenureGroup')['Churn'].apply(lambda x: (x == 'Yes').mean()).reset_index()\n",
        "\n",
        "# Plot churn rate by tenure group\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(data=churn_rate, x='TenureGroup', y='Churn')\n",
        "\n",
        "plt.title('Churn Rate by Tenure Group (Months)')\n",
        "plt.xlabel('Tenure Group (Months)')\n",
        "plt.ylabel('Churn Rate')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I picked this bar chart because grouping tenure into intervals makes it easy to see how churn rates change as customers stay longer. It clearly highlights which tenure groups are most at risk of churning, helping target retention efforts effectively."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The chart shows that customers with **shorter tenure periods (0-12 months)** have the highest churn rates, indicating they are more likely to leave early. Churn rates decrease as tenure increases, suggesting longer-term customers tend to stay loyal. This highlights the importance of focusing on early engagement and retention efforts."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, these insights can create a positive business impact by showing that targeting new customers within their first year is crucial to reducing churn. By improving onboarding, offering early incentives, or enhancing customer support during this period, the company can increase retention, boost customer lifetime value, and strengthen long-term revenue."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='InternetService', y='MonthlyCharges', hue='Churn', data=df, palette={'Yes':'red', 'No':'green'})\n",
        "\n",
        "plt.title('Monthly Charges by Internet Service and Churn')\n",
        "plt.xlabel('Internet Service')\n",
        "plt.ylabel('Monthly Charges')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "picked the boxplot because it shows the distribution of monthly charges across different internet service types while highlighting differences between customers who churn and those who stay. This helps reveal if higher or lower charges in each service category influence churn behavior."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "\n",
        "The chart shows that customers with **Fiber optic** internet tend to have higher monthly charges and a higher churn rate compared to DSL or No internet service. It suggests that higher costs with Fiber optic may be linked to greater churn, while customers without internet service or with DSL usually pay less and churn less."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "\n",
        "The insights can help create a positive business impact by identifying that customers with Fiber optic internet, who generally pay higher monthly charges, have a higher churn rate. This allows the company to focus on improving service quality, customer support, or pricing for Fiber optic users to reduce churn and increase retention.\n",
        "\n",
        "***\n",
        "\n",
        "Regarding negative growth, if the company does not address the higher churn among Fiber optic customers—potentially due to dissatisfaction with cost or service—it risks losing valuable, higher-paying customers. This could lead to revenue decline and increased customer acquisition costs, negatively impacting growth."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "# Prepare data: counts of churn by SeniorCitizen status\n",
        "counts = df.groupby(['SeniorCitizen', 'Churn']).size().unstack(fill_value=0)\n",
        "\n",
        "# Calculate proportions\n",
        "proportions = counts.div(counts.sum(axis=1), axis=0)\n",
        "\n",
        "# Plot stacked area chart\n",
        "proportions.plot(kind='area', stacked=True, figsize=(8, 6), cmap='RdYlGn_r')\n",
        "\n",
        "plt.title('Churn Proportion by Senior Citizen Status')\n",
        "plt.xlabel('Senior Citizen (0 = No, 1 = Yes)')\n",
        "plt.ylabel('Proportion of Customers')\n",
        "plt.xticks([0, 1])\n",
        "plt.legend(title='Churn')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "I picked the stacked area chart because it visually represents the proportion of churned and retained customers within each senior citizen group in a smooth, continuous way. Unlike bar charts, it allows easy comparison of relative shares between groups while highlighting overall distribution trends without clutter. This makes it clear how churn differs by age without focusing on exact counts."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The chart shows that the **proportion of churned customers is slightly higher among senior citizens** compared to non-senior customers. This suggests that senior citizens are somewhat more likely to leave the service, indicating a need for targeted retention efforts for this group."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "the insights can help create a positive business impact by identifying that senior citizens have a higher churn proportion. The company can develop specialized retention strategies for senior customers, such as tailored service plans, enhanced support, or loyalty incentives, to reduce churn and improve revenue stability.\n",
        "\n",
        "***\n",
        "\n",
        "On the other hand, if the higher churn among senior citizens is not addressed, it could contribute to **negative growth** by losing a valuable customer segment that may have distinct needs and potentially higher lifetime value. Ignoring this could increase customer acquisition costs and reduce overall profitability."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Select relevant numerical columns\n",
        "numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = df[numerical_cols].corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "\n",
        "plt.title('Correlation Heatmap of Numerical Variables')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "I picked the correlation heatmap because it provides a clear, visual summary of how key numerical variables like tenure, monthly charges, and total charges relate to each other. Understanding these relationships helps identify which factors move together and may influence customer churn, guiding more effective analysis and decision-making."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The heatmap reveals a strong positive correlation between **tenure and total charges**, meaning customers who stay longer generally accumulate higher total charges. Monthly charges have a weaker correlation with tenure and total charges, suggesting that monthly fees vary less predictably with customer longevity. These relationships indicate that customer duration is a key factor in overall revenue and potentially churn dynamics."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "\n",
        "Yes, these insights can create a positive business impact by emphasizing the importance of retaining customers for longer tenure periods to maximize total charges and revenue. Understanding the strong link between tenure and total charges encourages investment in customer loyalty programs and service improvements that reduce churn.\n",
        "\n",
        "***\n",
        "\n",
        "However, if customers leave early (short tenure), it leads to lower total charges and revenue, which can cause **negative growth**. This highlights the risk of insufficient retention efforts resulting in lost revenue and higher costs to replace churned customers."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "# Count customers by DeviceProtection and Churn\n",
        "device_churn_counts = df.groupby(['DeviceProtection', 'Churn']).size().reset_index(name='Count')\n",
        "\n",
        "# Plot the churn counts using barplot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(data=device_churn_counts, x='DeviceProtection', y='Count', hue='Churn', palette={'Yes':'blue', 'No':'orange'})\n",
        "\n",
        "plt.title('Customer Churn Count by Device Protection Subscription')\n",
        "plt.xlabel('Device Protection')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.legend(title='Churn')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "Yes, this chart is useful for your project because it provides insight into whether offering device protection influences customer churn. Understanding this relationship can help identify if adding or promoting device protection services reduces churn and improves customer retention, which is valuable for business strategy and decisions."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The chart shows that customers **without device protection have a higher churn count** compared to those who have device protection. This suggests that subscribing to device protection is associated with better customer retention and lower likelihood of churning."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, the insights can create a positive business impact by highlighting the importance of device protection in reducing customer churn. Promoting device protection services or bundling them with other offerings can enhance customer satisfaction and loyalty, leading to higher retention and steady revenue growth.\n",
        "\n",
        "***\n",
        "\n",
        "On the other hand, if the company neglects device protection or fails to communicate its value, customers may perceive less benefit from the service, potentially increasing churn rates. This could result in **negative growth** due to loss of customers who might seek competitors offering better device protection options."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(x='Contract', y='MonthlyCharges', hue='Churn', data=df, split=True, palette={'No':'blue', 'Yes':'orange'})\n",
        "\n",
        "plt.title('Monthly Charges Distribution by Contract Type and Churn')\n",
        "plt.xlabel('Contract Type')\n",
        "plt.ylabel('Monthly Charges')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I picked the violin plot because it provides a detailed view of the distribution and density of monthly charges across different contract types while simultaneously showing differences between churned and retained customers. This chart reveals not just central tendencies but also the shape and spread of the data, helping identify patterns or variability that simpler plots might miss."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The chart shows that customers with month-to-month contracts tend to have a wider range of monthly charges and a higher density of churned customers paying mid to high monthly fees. In contrast, one-year and two-year contract customers generally have lower churn rates and more consistent monthly charges. This suggests that customers on short-term contracts with higher fees are more likely to leave, indicating a need for targeted retention efforts or pricing adjustments in this segment."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "\n",
        "Yes, the insights can help create a positive business impact by identifying that customers on month-to-month contracts with higher monthly charges are more likely to churn. The company can use this information to design targeted retention strategies, such as offering loyalty discounts or flexible contract options to these customers, thereby reducing churn and increasing revenue stability.\n",
        "\n",
        "***\n",
        "\n",
        "Conversely, if these high-risk customers are not addressed, the company may experience **negative growth** due to losing customers who pay higher monthly fees. High churn in this segment can lead to decreased revenue and increased costs for acquiring new customers to replace those lost, ultimately impacting profitability."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='InternetService', y='TotalCharges', hue='Churn', data=df, palette={'Yes':'red', 'No':'green'})\n",
        "\n",
        "plt.title('Total Charges Distribution by Internet Service and Churn')\n",
        "plt.xlabel('Internet Service')\n",
        "plt.ylabel('Total Charges')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I picked the boxplot because it clearly displays the distribution of total charges for different internet service types while simultaneously comparing churned and retained customers. This makes it easy to see variations, medians, and outliers in spending across service categories, helping identify cost-related factors that may influence churn."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "\n",
        "The chart shows that customers with Fiber optic internet generally have higher total charges compared to those with DSL or no internet service. Among Fiber optic users, churned customers tend to have slightly lower total charges than retained ones, possibly indicating early churn before accumulating higher charges. This suggests that high-spending Fiber optic customers who leave may do so relatively early in their subscription, highlighting a risk segment for retention efforts."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, the insights can help create a positive business impact by identifying that Fiber optic customers with lower total charges are more likely to churn early. The company can focus retention efforts on these customers through targeted offers, improved service quality, or early engagement programs to increase their lifetime value.\n",
        "\n",
        "***\n",
        "\n",
        "However, if this early churn among Fiber optic customers is not addressed, it can lead to **negative growth** because the company loses potentially high-value customers before they generate substantial revenue, increasing acquisition costs and reducing overall profitability."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "# Prepare data by counting occurrences for each combination\n",
        "sunburst_data = df.groupby(['Contract', 'PaymentMethod', 'Churn']).size().reset_index(name='count')\n",
        "\n",
        "# Create sunburst chart\n",
        "fig = px.sunburst(\n",
        "    sunburst_data,\n",
        "    path=['Contract', 'PaymentMethod', 'Churn'],\n",
        "    values='count',\n",
        "    color='Churn',\n",
        "    color_discrete_map={'Yes':'red', 'No':'green'},\n",
        "    title='Contract and Payment Method Distribution by Churn'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I picked the sunburst chart because it effectively visualizes hierarchical relationships between multiple categorical variables—contract type, payment method, and churn status—in a single, easy-to-understand graphic. This helps reveal complex interaction patterns and customer segments that contribute to churn, which might be harder to see in separate charts."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The sunburst chart reveals which combinations of contract types and payment methods have higher proportions of churned customers. For example, it may show that customers with month-to-month contracts who pay by electronic check tend to churn more than those with longer contracts or other payment methods. This insight helps pinpoint specific customer segments that are at higher risk of leaving, enabling targeted retention efforts."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, the gained insights can help create a positive business impact by enabling the company to focus retention strategies on high-risk customer segments identified in the chart—such as those on month-to-month contracts paying via electronic check. By tailoring offers, payment options, or contract incentives to these groups, the company can reduce churn and improve revenue stability.\n",
        "\n",
        "***\n",
        "\n",
        "However, if these insights are ignored, the company risks continued **negative growth** due to losing valuable customers who may be more likely to churn because of contract terms or payment methods that do not meet their preferences. This leads to higher acquisition costs and revenue loss from churned customers."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "# Create a contingency table from the data\n",
        "contingency = df.groupby(['InternetService', 'Contract', 'Churn']).size()\n",
        "\n",
        "# Convert to dictionary with keys as tuples for mosaic plot\n",
        "data_dict = contingency.to_dict()\n",
        "\n",
        "# Plot mosaic\n",
        "plt.figure(figsize=(12, 8))\n",
        "mosaic(data_dict, gap=0.01, title='Internet Service, Contract Type, and Churn Relationship',\n",
        "       properties=lambda key: {'color': 'orange' if 'Yes' in key else 'skyblue'})\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        " picked the mosaic plot because it visually represents the proportions and interactions between multiple categorical variables simultaneously—in this case, Internet Service, Contract Type, and Churn. Its area-based layout makes it easy to compare group sizes and reveal complex relationships in the data that might be harder to interpret with simpler charts."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "\n",
        "The mosaic plot reveals that customers with month-to-month contracts and Fiber optic internet have a higher proportion of churn compared to other groups. It also shows that customers with one-year or two-year contracts, regardless of internet service, tend to have lower churn rates. This indicates that contract length and type of internet service together significantly influence churn behavior.The mosaic plot reveals that customers with month-to-month contracts and Fiber optic internet have a higher proportion of churn compared to other groups. It also shows that customers with one-year or two-year contracts, regardless of internet service, tend to have lower churn rates. This indicates that contract length and type of internet service together significantly influence churn behavior."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, the insights can help create a positive business impact by highlighting that customers with month-to-month contracts and Fiber optic internet are more likely to churn. The company can focus retention strategies on these high-risk segments by offering incentives for longer contracts or improving Fiber optic service quality, which can reduce churn and increase revenue stability.\n",
        "\n",
        "***\n",
        "\n",
        "Conversely, if these high-churn segments are not addressed, the company risks **negative growth** from losing valuable customers who may generate significant revenue over time. High churn in these groups increases customer acquisition costs and disrupts revenue predictability, negatively affecting overall profitability."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "# Select numerical columns relevant for correlation analysis\n",
        "numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = df[numeric_cols].corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "This heatmap helps visualize how features like tenure, monthly charges, and total charges relate to each other, providing insights for feature selection and understanding potential drivers of churn"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The correlation heatmap shows that tenure and total charges have a strong positive correlation, indicating that customers who stay longer tend to accumulate higher total charges. Monthly charges have a moderate positive correlation with total charges but a weak correlation with tenure. These relationships suggest that longer-tenured customers generate more revenue overall, while monthly charges are relatively independent of how long a customer stays. Understanding these correlations can help focus retention strategies on customers with high total charges and tenure."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select relevant numeric columns plus churn for hue\n",
        "cols = ['tenure', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
        "\n",
        "# Create pair plot with hue as churn\n",
        "sns.pairplot(df[cols], hue='Churn', diag_kind='kde', palette={'Yes':'red', 'No':'green'})\n",
        "\n",
        "plt.suptitle('Pair Plot of Numerical Features Colored by Churn', y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "I picked the pair plot because it provides a comprehensive visual summary of relationships and distributions among multiple numerical variables simultaneously, while highlighting differences between churned and retained customers. This makes it easier to detect patterns, clusters, or separations that can inform churn prediction and targeting strategies."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "\n",
        "\n",
        "The pair plot reveals that churned customers generally have shorter tenure and lower total charges compared to retained customers, indicating early churn in the customer lifecycle. It also shows that monthly charges tend to be slightly higher for some churned customers, especially in combination with low tenure. These patterns highlight that customers with short tenure and higher monthly costs are more likely to churn, suggesting key focus areas for retention efforts."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "**Hypothetical Statement 1**\n",
        "Customers with month-to-month contracts are more likely to churn than those with longer-term contracts.\n",
        "\n",
        "**Hypothetical Statement 2**\n",
        "Customers with Fiber optic internet service have a higher churn rate compared to customers with DSL or no internet service.\n",
        "\n",
        "**Hypothetical Statement 3**\n",
        "Churned customers have significantly shorter tenure than retained customers.\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0)**: There is no association between contract type (month-to-month vs. longer-term) and customer churn.\n",
        "\n",
        "**Alternate Hypothesis (H1)**: There is an association between contract type and customer churn."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "df['Contract_Simplified'] = df['Contract'].apply(lambda x: 'Month-to-month' if x == 'Month-to-month' else 'Longer-term')\n",
        "contingency_table = pd.crosstab(df['Contract_Simplified'], df['Churn'])\n",
        "\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi-square statistic: {chi2}\")\n",
        "print(f\"P-value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "* Chi-Square Test of Independence.\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Because both variables (**contract type and churn**) are categorical, and the Chi-Square test is designed to determine whether there is a statistically significant association between two categorical variables."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Research Hypothesis**: Customers with Fiber optic internet service have a higher churn rate compared to customers with DSL or no internet service.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Null Hypothesis (H0)**:\n",
        "There is no association between type of internet service (Fiber optic, DSL, No) and customer churn.\n",
        "(In other words, churn rates are independent of internet service type.)\n",
        "\n",
        "**Alternate Hypothesis (H1)**:\n",
        "There is an association between internet service type and customer churn.\n",
        "(Customers with Fiber optic internet have different churn rates compared to those with DSL or no internet service.)"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Create contingency table of InternetService vs Churn\n",
        "contingency_table = pd.crosstab(df['InternetService'], df['Churn'])\n",
        "\n",
        "# Perform Chi-Square Test\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi-square statistic: {chi2}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The statistical test performed to obtain the p-value is the **Chi-Square Test of Independence**."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The **Chi-Square Test of Independence** was chosen because both variables—Internet Service type (Fiber optic, DSL, No) and Churn status (Yes, No)—are categorical. This test is specifically designed to determine whether there is a statistically significant association or independence between two categorical variables by comparing observed and expected frequencies."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Research Hypothesis**: Churned customers have significantly shorter tenure than retained customers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Null Hypothesis (H0)**:\n",
        "The mean tenure of churned customers is equal to the mean tenure of retained customers.\n",
        "\n",
        "**Alternate Hypothesis (H1)**:\n",
        "The mean tenure of churned customers is different (specifically, shorter) than the mean tenure of retained customers."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Separate tenure data for churned and retained customers\n",
        "tenure_churned = df[df['Churn'] == 'Yes']['tenure']\n",
        "tenure_retained = df[df['Churn'] == 'No']['tenure']\n",
        "\n",
        "# Perform two-sample t-test (assuming unequal variances)\n",
        "t_stat, p_value = ttest_ind(tenure_churned, tenure_retained, equal_var=False)\n",
        "\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The statistical test performed to obtain the p-value is the **Independent Two-Sample t-Test** (Welch’s t-test)."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The **Independent Two-Sample t-Test** was chosen because it compares the means of a continuous variable (tenure) between two independent groups (churned and retained customers). It is appropriate for testing whether there is a statistically significant difference in average tenure between these two groups. Welch’s t-test version is used to account for unequal variances."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Identify missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "\n",
        "# For categorical columns use mode imputation:\n",
        "\n",
        "df['TenureGroup'].fillna(df['TenureGroup'].mode()[0], inplace=True)\n",
        "\n",
        "# For numerical columns with skewed distribution or outliers, use median imputation:\n",
        "\n",
        "df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
        "\n",
        "\n",
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "The primary missing value imputation techniques used here are **mode imputation** for categorical columns and **median (or mean) imputation** for numerical columns. Each technique is selected based on the nature of the data and the type of missingness in the feature.\n",
        "\n",
        "***\n",
        "\n",
        "## Techniques Used\n",
        "\n",
        "### 1. Mode Imputation (Categorical Data)\n",
        "- **Used For:** Columns like 'TenureGroup', which are categorical.\n",
        "- **Why:** Mode imputation replaces missing values with the most frequently occurring value in the column. This technique preserves the most common category without introducing artificial values and is standard for categorical data where no logical numerical summary exists.[1][2]\n",
        "\n",
        "### 2. Median Imputation (Numerical Data)\n",
        "- **Used For:** Numerical columns like 'TotalCharges', especially those with skewed distributions or outliers.\n",
        "- **Why:** Median imputation replaces missing values with the median, which is less sensitive to outliers or skewness than the mean. This results in a more robust imputation, retaining the integrity of the distribution.[2][3]\n",
        "\n",
        "### 3. Mean Imputation (Numerical Data)\n",
        "- **Used For:** Numerical columns with a roughly normal (symmetric) distribution and no significant outliers.\n",
        "- **Why:** Mean imputation is straightforward and effective when data is symmetrically distributed. It fills missing values with the column average, which works well if the missingness is random and the data isn’t heavily skewed.[4][2]\n",
        "\n",
        "***\n",
        "\n",
        "# Selection Rationale\n",
        "\n",
        "- **Categorical features:** Used mode, because categorical data has no inherent order or numerical average, and filling with the most prevalent value is least disruptive.\n",
        "- **Numerical features (with outliers/skewness):** Used median to minimize distortion from extreme values.\n",
        "- **Numerical features (normal distribution):** Used mean where appropriate, for simplicity and efficiency, but only when distribution allowed.\n",
        "\n",
        "Each technique was chosen to fit both the **data type** and the **distribution** of the feature, ensuring imputations are both statistically sound and contextually appropriate.\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Box Plote\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['tenure'], df['MonthlyCharges'])\n",
        "plt.xlabel('Tenure')\n",
        "plt.ylabel('Monthly Charges')\n",
        "plt.title('Scatter Plot of Tenure vs Monthly Charges')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Histogram\n",
        "\n",
        "plt.hist(df['tenure'], bins=10, edgecolor='black')\n",
        "plt.xlabel('Tenure')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Tenure')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# IQR Outlier Removal on numeric columns\n",
        "q1 = numeric_df.quantile(0.25)\n",
        "q3 = numeric_df.quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "# Detect outliers in any numeric column\n",
        "outliers_iqr = numeric_df[((numeric_df < lower_bound) | (numeric_df > upper_bound)).any(axis=1)]\n",
        "\n",
        "# DataFrame excluding outliers for numeric columns\n",
        "df_no_outliers_iqr = numeric_df[((numeric_df >= lower_bound) & (numeric_df <= upper_bound)).all(axis=1)]\n",
        "\n",
        "print(\"\\nOutliers detected using IQR Outlier Removal:\")\n",
        "print(outliers_iqr)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The primary outlier treatment techniques commonly used and recommended are as follows, along with the rationale behind choosing each:\n",
        "\n",
        "***\n",
        "\n",
        "## Outlier Treatment Techniques Used\n",
        "\n",
        "### 1. Outlier Detection with IQR Method\n",
        "- **Why Used:** IQR (Interquartile Range) method is robust and non-parametric, meaning it does not assume normality of data. It is effective for identifying outliers by defining acceptable value ranges based on quartiles.\n",
        "- **Usage:** Detects data points outside the range $$[Q1 - 1.5 \\times IQR, Q3 + 1.5 \\times IQR]$$ as outliers.\n",
        "- **Benefit:** Simple to implement and interpretable for many datasets with skewed or non-normal distributions.\n",
        "\n",
        "### 2. Outlier Removal\n",
        "- **Why Used:** Complete removal of rows containing outliers ensures that extreme values do not bias model training or statistical analysis when these points are genuinely errors or anomalies.\n",
        "- **Usage:** After detecting outliers with IQR or other methods, remove these points.\n",
        "- **Benefit:** Can improve model accuracy and stability but risks losing potentially useful data if outliers are legitimate.\n",
        "\n",
        "### 3. Outlier Capping (Winsorizing)\n",
        "- **Why Used:** Instead of removing outliers, extreme values are capped to the nearest acceptable boundary (e.g., capping values beyond Q3 + 1.5 IQR to that threshold).\n",
        "- **Benefit:** Retains all data points but reduces the influence of extreme outliers, preserving dataset size while reducing skew.\n",
        "\n",
        "### 4. Visualization for Outlier Insight\n",
        "- **Why Used:** Visualizations like box plots and scatter plots allow intuitive identification of outliers and understanding their relationship with other features.\n",
        "- **Benefit:** Provides context for deciding the appropriate treatment method depending on whether outliers are errors, rare but valid values, or expected variability.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Since nominal variables are already one-hot encoded, skip one-hot encoding step\n"
      ],
      "metadata": {
        "id": "Wjnz-w4f3tyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Since nominal variables are already one-hot encoded, skip one-hot encoding step\n",
        "\n",
        "# Ordinal encoding for 'Contract_Simplified' if not already encoded\n",
        "if 'Contract_Simplified_encoded' not in df.columns:\n",
        "    ordinal_mapping = {'Month-to-month': 0, 'One year': 1, 'Two year': 2}\n",
        "    df['Contract_Simplified_encoded'] = df['Contract_Simplified'].map(ordinal_mapping)\n",
        "\n",
        "print(\"Categorical encoding is complete.\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The categorical encoding techniques used mainly include:\n",
        "\n",
        "1. **One-Hot Encoding**  \n",
        "   - Converts nominal categorical features into binary columns, one for each category.  \n",
        "   - Suitable when there is no ordinal relationship between categories.  \n",
        "   - Helps models like linear regression, logistic regression, and neural networks which assume numerical input without ordinal bias.  \n",
        "   - Example: encoding 'InternetService' into 'InternetService_Fiber optic', 'InternetService_No', etc.  \n",
        "\n",
        "2. **Label Encoding (Ordinal Encoding)**  \n",
        "   - Maps ordinal categorical features to integer values preserving the order.  \n",
        "   - Suitable for features with inherent order (e.g., 'Contract_Simplified': Month-to-month < One year < Two year).  \n",
        "   - Useful for tree-based models or others that can interpret the ordering.\n",
        "\n",
        "***\n",
        "\n",
        "### Why these techniques were used?\n",
        "\n",
        "- **One-Hot Encoding** allows models that assume features are independent categories to treat them as separate entities without introducing misleading ordinal relationships.  \n",
        "- **Label Encoding** is beneficial for ordinal variables because it preserves their ranking, enabling models to leverage that ordering information.\n",
        "\n",
        "***\n",
        "\n",
        "\n",
        "- Studies show **One-Hot Encoding** works well with models like logistic regression and neural networks, while **target encoding or label encoding** variants often benefit tree-based models (e.g., Random Forest, XGBoost).  \n",
        "- High cardinality categorical variables may require advanced encoders like Target Encoding for scalability and performance."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "\n",
        "# Sample DataFrame with a text column named 'customer_feedback'\n",
        "df = pd.DataFrame({\n",
        "    'customer_feedback': [\n",
        "        \"I can't wait for the new service!\",\n",
        "        \"She's not happy with the billing.\",\n",
        "        \"We'll call support tomorrow.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Apply contraction expansion on 'customer_feedback' column\n",
        "df['feedback_expanded'] = df['customer_feedback'].apply(contractions.fix)\n",
        "\n",
        "print(df[['customer_feedback', 'feedback_expanded']])"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "\n",
        "# Example DataFrame with a text column 'customer_feedback'\n",
        "df = pd.DataFrame({\n",
        "    'customer_feedback': [\n",
        "        \"I CAN'T wait for the New Service!\",\n",
        "        \"She's NOT happy with the Billing.\",\n",
        "        \"We'll CALL support Tomorrow.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Apply lowercasing to 'customer_feedback' column\n",
        "df['feedback_lower'] = df['customer_feedback'].str.lower()\n",
        "\n",
        "print(df[['customer_feedback', 'feedback_lower']])\n",
        "\n",
        "\n",
        "\n",
        "# Apply lowercasing first\n",
        "df['feedback_lower'] = df['customer_feedback'].str.lower()\n",
        "\n",
        "# Expand contractions on the lowercase text\n",
        "df['feedback_expanded'] = df['feedback_lower'].apply(contractions.fix)\n",
        "\n",
        "print(df[['customer_feedback', 'feedback_lower', 'feedback_expanded']])"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "# Sample DataFrame with text column 'feedback_expanded'\n",
        "df = pd.DataFrame({\n",
        "    'feedback_expanded': [\n",
        "        \"i cannot wait for the new service!\",\n",
        "        \"she is not happy with the billing.\",\n",
        "        \"we will call support tomorrow.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Remove punctuation\n",
        "df['feedback_no_punct'] = df['feedback_expanded'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
        "\n",
        "print(df[['feedback_expanded', 'feedback_no_punct']])"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'feedback_no_punct': [\n",
        "        \"Check this out: https://example.com Offer123 is valid!\",\n",
        "        \"Visit our site http://abc123.org for more info.\",\n",
        "        \"Call us at 123service or go to www.support24x7.com.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Define a function to remove URLs and words containing digits\n",
        "def clean_text(text):\n",
        "    # Remove URLs (fix regex pattern)\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "    # Remove words with digits\n",
        "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply the function to the column\n",
        "df['feedback_clean'] = df['feedback_no_punct'].apply(clean_text)\n",
        "\n",
        "print(df[['feedback_no_punct', 'feedback_clean']])"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'feedback_clean': [\n",
        "        \"Check this out is valid\",\n",
        "        \"Visit our site for more info\",\n",
        "        \"Call us at or go to\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Download NLTK stopwords (run this once)\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords and extra spaces\n",
        "def remove_stopwords_and_spaces(text):\n",
        "    # Tokenize the text\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    # Join the tokens and remove extra spaces\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply to the column\n",
        "df['feedback_final'] = df['feedback_clean'].apply(remove_stopwords_and_spaces)\n",
        "\n",
        "print(df[['feedback_clean', 'feedback_final']])\n",
        "\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'text': [\n",
        "        \"This   is    an example    text.\",\n",
        "        \"Another     example with   irregular   spaces.\",\n",
        "        \"  Leading and trailing    spaces   \"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Function to remove extra whitespaces\n",
        "def remove_extra_spaces(text):\n",
        "    # Replace multiple spaces/newlines/tabs with a single space\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply to the 'text' column\n",
        "df['text_clean'] = df['text'].apply(remove_extra_spaces)\n",
        "\n",
        "print(df[['text', 'text_clean']])\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "\n",
        "\n",
        "# Load pretrained model and tokenizer\n",
        "model_name = \"tuner007/pegasus_paraphrase\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def paraphrase(text, num_return_sequences=3, num_beams=5):\n",
        "    inputs = tokenizer([text], truncation=True, padding='longest', return_tensors='pt')\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=60,\n",
        "        num_beams=num_beams,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        temperature=1.5,\n",
        "    )\n",
        "    paraphrased_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return paraphrased_texts\n",
        "\n",
        "# Example usage\n",
        "sentence = \"Customer churn prediction helps telecom companies retain customers.\"\n",
        "paraphrases = paraphrase(sentence)\n",
        "\n",
        "for i, para in enumerate(paraphrases, 1):\n",
        "    print(f\"Paraphrase {i}: {para}\")"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\n",
        "# Download the tokenizer models (run once)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Tokenization is essential in NLP. Let's learn how to tokenize!\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "# Download necessary data (run once)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "text = \"running runner ran better\"\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"Stemmed Words:\", stemmed_words)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "In the context of your textual data preprocessing, the text normalization techniques used include:\n",
        "\n",
        "1. **Lowercasing**  \n",
        "   - Converts all text to lowercase to standardize words that differ only by case (e.g., “Internet” vs “internet”).  \n",
        "   - Simplifies vocabulary and helps models treat words uniformly.\n",
        "\n",
        "2. **Removing Punctuation**  \n",
        "   - Eliminates punctuation marks that usually do not contribute to meaning in tasks like sentiment analysis or clustering.  \n",
        "   - Reduces noise and simplifies tokenization.\n",
        "\n",
        "3. **Removing Stopwords**  \n",
        "   - Removes common words (e.g., “the”, “is”, “and”) that add little semantic value.  \n",
        "   - Focuses analysis on meaningful words to improve model performance.\n",
        "\n",
        "4. **Removing URLs and Words Containing Digits**  \n",
        "   - Removes noisy elements (like links or product codes) that may not help analysis.  \n",
        "   - Cleans data for more relevant textual features.\n",
        "\n",
        "***\n",
        "\n",
        "**Why these techniques?**\n",
        "\n",
        "These techniques are chosen because they **standardize and clean the text**, making it more consistent and less noisy. This improves the effectiveness and accuracy of subsequent NLP steps such as tokenization, vectorization, and model training.\n",
        "\n"
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "\n",
        "\n",
        "# Download required NLTK models (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "\n",
        "documents = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog sat on the log.\",\n",
        "    \"Cats and dogs are pets.\"\n",
        "]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(tfidf_vectors.toarray())\n",
        "print(tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.   \n",
        "\n",
        "\n",
        "\n",
        "In the context of your textual data preprocessing, the recommended and commonly used text vectorization techniques include:\n",
        "\n",
        "1. **TF-IDF (Term Frequency-Inverse Document Frequency)**  \n",
        "   - Converts text into weighted word-frequency vectors that highlight important words relevant to documents while down-weighting common words.  \n",
        "   - It balances the occurrence of words across documents, improving feature quality for classification or clustering.  \n",
        "   - Suitable for structured NLP problems with moderate dataset sizes and provides interpretable features.\n",
        "\n",
        "2. **Word Embeddings (e.g., Word2Vec or GloVe)**  \n",
        "   - Maps words to dense vectors capturing semantic relationships and context similarities.  \n",
        "   - Useful when semantic meaning is crucial or when working with deep learning models.  \n",
        "   - Embeddings generalize better across text variations.\n",
        "\n",
        "***\n",
        "\n",
        "### Why these techniques?\n",
        "\n",
        "- **TF-IDF** was often chosen for its balance between simplicity, interpretability, and effectiveness across many traditional NLP tasks. It works well with machine learning models like logistic regression or random forest.\n",
        "\n",
        "- **Word Embeddings** are preferred when deeper semantic understanding is needed, such as for sentiment analysis or chatbots.\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'tenure': [12, 24, 36, 48, 60],\n",
        "    'MonthlyCharges': [30, 45, 40, 35, 50],\n",
        "    'TotalCharges': [360, 1080, 1440, 1680, 3000],\n",
        "    'OldFeature': [1, 1, 1, 1, 1]  # Example highly correlated or redundant feature\n",
        "})\n",
        "\n",
        "# Create new feature\n",
        "df['AverageMonthlyCharge'] = df['TotalCharges'] / df['tenure']\n",
        "\n",
        "# Check correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "print(\"Correlation matrix:\\n\", corr_matrix)\n",
        "\n",
        "# Drop highly correlated features (example threshold >0.9)\n",
        "to_drop = [col for col in corr_matrix.columns if any((corr_matrix[col] > 0.9) & (corr_matrix[col] < 1.0))]\n",
        "df = df.drop(columns=to_drop)\n",
        "\n",
        "print(\"\\nData after dropping highly correlated features:\\n\", df)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Data/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "X = df.drop(columns=['customerID', 'Churn'])\n",
        "\n",
        "# One-hot encode categorical features\n",
        "X_encoded = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Map target variable to binary\n",
        "y = df['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Logistic Regression model\n",
        "model = LogisticRegression(max_iter=300, solver='liblinear')\n",
        "\n",
        "# Select top 15 features based on model feature importance\n",
        "sfm = SelectFromModel(model, max_features=15)\n",
        "sfm.fit(X_encoded, y)\n",
        "\n",
        "# Get selected features\n",
        "selected_features = X_encoded.columns[sfm.get_support()]\n",
        "print(\"Selected Features:\", list(selected_features))\n",
        "\n",
        "# Create final dataset with selected features\n",
        "X_selected = X_encoded[selected_features]\n"
      ],
      "metadata": {
        "id": "HCwO55i1GUIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The feature selection method used in your code is **SelectFromModel** with a Logistic Regression model as the estimator.\n",
        "\n",
        "### What is SelectFromModel?\n",
        "\n",
        "- **SelectFromModel** is an embedded feature selection method.\n",
        "- It uses the importance weights or coefficients assigned by a model (e.g., Logistic Regression coefficients) to select the most relevant features.\n",
        "- You can specify the maximum number of features to keep, like `max_features=15` in your code.\n",
        "- It is straightforward and efficient because it selects features in one pass without iterative model fitting like recursive methods.\n",
        "\n",
        "### Why SelectFromModel?\n",
        "\n",
        "- It provides a **fast and practical way** to reduce dimensionality based on model-driven importance.\n",
        "- Useful when you want to **limit features to a certain number** for better interpretability or computational reasons.\n",
        "- Compared to recursive methods like RFECV, it is **much faster** and easier to use on larger datasets.\n",
        "\n",
        "\n",
        "\n",
        "### Summary\n",
        "\n",
        "You used **SelectFromModel with Logistic Regression** because it:\n",
        "\n",
        "- Selects the top 15 most important features based on the model.\n",
        "- Is computationally efficient and simple.\n",
        "- Works well when you want a quick feature selection without exhaustive search.\n",
        "\n",
        "If needed, recursive methods like RFECV can be used for potentially more optimized but costlier feature selection.\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.    \n",
        "\n",
        "\n",
        "\n",
        "The important features found through your `SelectFromModel` Logistic Regression approach are:\n",
        "\n",
        "- **Categorical Features:**\n",
        "  - `InternetService_Fiber optic`  \n",
        "  - `Contract_Two year`\n",
        "  \n",
        "- **Multiple numeric bins derived from `TotalCharges`** such as:  \n",
        "   TotalCharges_19.6, TotalCharges_19.9, TotalCharges_20.1, TotalCharges_20.15, TotalCharges_20.2, TotalCharges_20.5, TotalCharges_20.9, TotalCharges_259.8, TotalCharges_288.05, TotalCharges_45.1, TotalCharges_45.7, TotalCharges_50.45, TotalCharges_740.3\n",
        "\n",
        "***\n",
        "\n",
        "### Why these features are important:\n",
        "\n",
        "1. **InternetService_Fiber optic:**  \n",
        "   - Fiber optic internet service is often linked to customer experience quality and pricing, which can strongly influence churn decisions.\n",
        "\n",
        "2. **Contract_Two year:**  \n",
        "   - Longer contracts (two years) usually indicate higher customer commitment, and this feature likely has predictive power on retention/churn behavior.\n",
        "\n",
        "3. **TotalCharges bins:**  \n",
        "   - TotalCharges represents the total amount a customer has been charged. Its variations help capture customer value and engagement over time — high or low total charges can correlate with churn likelihood.  \n",
        "   - However, multiple bins likely represent finely discretized charges, which could be simplified.\n",
        "\n",
        "\n",
        "### Note on Numeric Features\n",
        "\n",
        "- The many `TotalCharges` bins suggest detailed discretization of a continuous variable.  \n",
        "- Consider keeping `TotalCharges` as a continuous numeric feature for a simpler, potentially more generalizable model.\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "# Convert to numeric (coerce errors)\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "# Fill missing values with median\n",
        "df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "df[['TotalCharges', 'MonthlyCharges']] = scaler.fit_transform(df[['TotalCharges', 'MonthlyCharges']])\n",
        "\n",
        "# Example for encoding categorical features (if needed)\n",
        "# df = pd.get_dummies(df, columns=['gender', 'Contract'])\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes**, the data does need to be transformed, and StandardScaler is an excellent choice for scaling numerical features like TotalCharges and MonthlyCharges. Here’s why:\n",
        "\n",
        "\n",
        "------\n",
        "\n",
        "\n",
        "**Why use StandardScaler?**\n",
        "\n",
        "\n",
        "\n",
        "StandardScaler standardizes features by removing the mean and scaling to unit variance, transforming data such that each feature has a mean of 0 and standard deviation of 1.\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "**Why it’s important**:\n",
        "Many machine learning algorithms (e.g., Logistic Regression, SVM, Neural Networks) assume input features are on a similar scale and normally distributed. Features with widely different scales can negatively affect model training and convergence."
      ],
      "metadata": {
        "id": "oRskmbOlQrBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_num = df[['TotalCharges', 'MonthlyCharges']]\n",
        "X_scaled = scaler.fit_transform(X_num)\n",
        "\n",
        "print(X_scaled)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "\n",
        "**Min-Max Scaling** : I used Min-Max Scaling for your numeric features (TotalCharges, MonthlyCharges) because:  \n",
        "\n",
        " * It transforms features into a common bounded range [0,1].\n",
        "\n",
        "* It works well for your dataset after cleaning since telecom numeric data typically doesn't have extreme outliers.\n",
        "\n",
        "* It supports algorithms like Logistic Regression and neural nets by keeping feature values consistent and comparable.\n",
        "\n"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Yes, **dimensionality reduction** is often beneficial especially when:\n",
        "\n",
        "* Your dataset has a large number of features, especially after one-hot encoding categorical variables, which can cause feature explosion.\n",
        "\n",
        "* High dimensionality can lead to the curse of dimensionality, where the data becomes sparse and models may overfit or perform poorly.\n",
        "\n",
        "* It helps reduce computational complexity and training time.\n",
        "\n",
        "* It improves model interpretability by focusing on the most relevant information.\n",
        "\n",
        "* Can help remove redundant or noisy features that do not contribute significantly to predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "\n",
        "\n",
        "# Select numerical columns for scaling\n",
        "numeric_cols = ['TotalCharges', 'MonthlyCharges']\n",
        "\n",
        "# Convert to numeric if needed (handle non-numeric gracefully)\n",
        "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill missing numeric values with mean\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "\n",
        "# Scale numeric features with MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Perform PCA to retain 95% variance\n",
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Original number of features: {X_scaled.shape[1]}\")\n",
        "print(f\"Reduced number of features after PCA: {X_reduced.shape[1]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I used **Principal Component Analysis** (**PCA**) for dimensionality reduction because it efficiently reduces the number of features while retaining most of the important information (variance) in the data. PCA simplifies the dataset, removes redundancy, speeds up model training, and helps avoid overfitting by projecting the original features into a smaller set of uncorrelated components.\n",
        "\n",
        "PCA is widely used due to its balance of effectiveness and computational simplicity, making it a strong choice when dimensionality reduction is needed."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_selected, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The shapes you shared indicate:\n",
        "\n",
        "**X_train shape: (5634, 15)** means your training set has 5,634 samples and 15 features.\n",
        "\n",
        "**X_test shape: (1409, 15)** means your testing set has 1,409 samples and 15 features.\n",
        "\n",
        "**This is consistent with an 80:20 split of a dataset** with approximately 7,043 total samples (5634 + 1409) and 15 selected features."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Yes, the dataset is imbalanced.**\n",
        "\n",
        "\n",
        "* The class distribution in your test data shows 1035 samples of class 0 (no churn) and 374 samples of class 1 (churn).\n",
        "\n",
        "* This means about 74% of samples belong to the no churn class, and only about 26% belong to the churn class.\n",
        "\n",
        "* Such a difference between class sizes means the dataset is imbalanced, with one class (no churn) being much larger than the other (churn).\n",
        "\n",
        "* Imbalanced data often causes models to predict the majority class more accurately while struggling with the minority class, which is reflected in your model’s lower precision and recall for the churn class."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "\n",
        "# Initialize Logistic Regression with class weighting\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=300, solver='liblinear')\n",
        "\n",
        "# Train the model on existing training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on existing test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "To handle the imbalanced dataset, I used class weighting in the **Logistic Regression model.**\n",
        "\n",
        "**Why Class Weighting?**\n",
        "\n",
        "* It’s a simple and effective way to give more importance to the minority class (churn) during model training.\n",
        "\n",
        "* The model penalizes mistakes on churners more heavily, encouraging better detection despite their smaller numbers.\n",
        "\n",
        "* It avoids changing or augmenting the data itself, keeping the original distribution intact.\n",
        "\n",
        "* Suitable as a first step to handle imbalance with minimal complexity and risk of overfitting."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=300, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluate Model Performance\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The machine learning model used is **Logistic Regression**.\n",
        "\n",
        "\n",
        "\n",
        "### Why Logistic Regression?\n",
        "\n",
        "- It is a simple and widely used algorithm for binary classification problems like churn prediction (churn vs no churn).\n",
        "- The model estimates the probability of a customer churning based on input features by fitting a linear decision boundary.\n",
        "- Logistic Regression provides **interpretable results**, showing how each feature influences the probability of churn.\n",
        "- It is efficient for datasets with moderate numbers of features and offers fast training and prediction.\n",
        "- In this case, **class weighting** was applied to handle class imbalance, helping the model give more attention to customers who churn despite their fewer numbers.\n",
        "\n",
        "-----\n",
        "\n",
        "This makes Logistic Regression a solid baseline model for churn prediction tasks due to its balance of interpretability, simplicity, and effectiveness.\n",
        "\n",
        "Let me know if you want a detailed explanation of how logistic regression works!"
      ],
      "metadata": {
        "id": "cyWxaVECrHCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Metric scores (replace None with 0)\n",
        "metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy']\n",
        "class_0_scores = [0.86, 0.73, 0.79, 0.0]  # replaced None with 0\n",
        "class_1_scores = [0.47, 0.67, 0.56, 0.0]  # replaced None with 0\n",
        "overall_accuracy = [0.0, 0.0, 0.0, 0.714]  # zeros for first three, accuracy value for last\n",
        "\n",
        "# Bar width and positions\n",
        "bar_width = 0.25\n",
        "r1 = np.arange(len(metrics))\n",
        "r2 = [x + bar_width for x in r1]\n",
        "r3 = [x + bar_width for x in r2]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(r1, class_0_scores, color='blue', width=bar_width, edgecolor='grey', label='Class 0 (No churn)')\n",
        "plt.bar(r2, class_1_scores, color='orange', width=bar_width, edgecolor='grey', label='Class 1 (Churn)')\n",
        "plt.bar(r3, overall_accuracy, color='green', width=bar_width, edgecolor='grey', label='Overall Accuracy')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Evaluation Metrics', fontweight='bold', fontsize=12)\n",
        "plt.ylabel('Score', fontweight='bold', fontsize=12)\n",
        "plt.xticks([r + bar_width for r in range(len(metrics))], metrics)\n",
        "plt.ylim([0, 1])\n",
        "plt.title('Evaluation Metric Scores for Logistic Regression Model')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# Define model and parameters for GridSearchCV\n",
        "model = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'max_iter': [100, 200, 300]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluate Model Performance\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "J5F4SiGTCAkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Get metric scores for Random Forest predictions\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred_rf, average=None)\n",
        "accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "# Metrics and scores\n",
        "metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy']\n",
        "class_0_scores = [precision[0], recall[0], f1_score[0], 0]\n",
        "class_1_scores = [precision[1], recall[1], f1_score[1], 0]\n",
        "overall_accuracy = [0, 0, 0, accuracy]\n",
        "\n",
        "# Bar width and positions\n",
        "bar_width = 0.25\n",
        "r1 = np.arange(len(metrics))\n",
        "r2 = [x + bar_width for x in r1]\n",
        "r3 = [x + 2 * bar_width for x in r1]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(r1, class_0_scores, color='blue', width=bar_width, edgecolor='grey', label='Class 0 (No churn)')\n",
        "plt.bar(r2, class_1_scores, color='orange', width=bar_width, edgecolor='grey', label='Class 1 (Churn)')\n",
        "plt.bar(r3, overall_accuracy, color='green', width=bar_width, edgecolor='grey', label='Overall Accuracy')\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel('Evaluation Metrics', fontweight='bold', fontsize=12)\n",
        "plt.ylabel('Score', fontweight='bold', fontsize=12)\n",
        "plt.xticks([r + bar_width for r in range(len(metrics))], metrics)\n",
        "plt.ylim([0, 1])\n",
        "plt.title('Evaluation Metric Scores for Random Forest Model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "\n",
        "\n",
        "# Define model and hyperparameters grid\n",
        "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "best_rf_model = grid_search_rf.best_estimator_\n",
        "y_pred_rf_tuned = best_rf_model.predict(X_test)\n",
        "\n",
        "# Optional: print best parameters and classification report\n",
        "print(\"Best Parameters:\", grid_search_rf.best_params_)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf_tuned))\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "The hyperparameter optimization technique used is **GridSearchCV**.\n",
        "\n",
        "\n",
        "\n",
        "### Why use GridSearchCV for Hyperparameter Tuning?\n",
        "\n",
        "- **Exhaustive Search:** GridSearchCV tries all specified combinations of hyperparameters (like number of trees, max depth, min samples split for Random Forest) to find the best performing set.\n",
        "- **Cross-Validation:** It uses k-fold CV (here, 5 folds), which evaluates model performance reliably on multiple splits of training data, reducing overfitting risk.\n",
        "- **Optimization Metric:** The search optimizes an evaluation metric like F1-score that balances precision and recall, critical for imbalanced tasks such as churn prediction.\n",
        "- **Interpretability & Control:** You can narrow or expand the hyperparameter grid depending on resource/time constraints.\n",
        "- **Widely Supported & Easy to Implement:** Part of scikit-learn, GridSearchCV is straightforward to set up and integrates well with the existing pipeline.\n",
        "\n",
        "\n",
        "### Alternatives\n",
        "\n",
        "- **RandomizedSearchCV:** Searches random hyperparameter combinations; faster on large search spaces but less exhaustive.\n",
        "- **Bayesian Optimization, Hyperband, Genetic Algorithms:** More advanced methods that can find better hyperparameters with fewer evaluations but require more complex setup.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "When evaluating whether hyperparameter tuning has improved a machine learning model, the key is to compare the evaluation metrics before and after tuning. Here’s how to note and interpret improvements theoretically:\n",
        "\n",
        "\n",
        "### How to Identify Improvement\n",
        "\n",
        "1. **Compare Metrics Before and After Tuning:**\n",
        "   - Look at core metrics such as **Accuracy**, **Precision**, **Recall**, and **F1-score**.\n",
        "   - Pay special attention to performance on the **minority class** (e.g., churners in a churn prediction model), as improvements there often have greater business impact.\n",
        "\n",
        "2. **Improvements in Metrics Indicate Better Model:**\n",
        "   - An increase in **accuracy** means the overall predictions are more correct.\n",
        "   - Better **precision** for the positive class means fewer false positives.\n",
        "   - Better **recall** indicates fewer false negatives, important for catching actual churners.\n",
        "   - Higher **F1-score** shows a better balance between precision and recall.\n",
        "\n",
        "3. **Evaluate Trade-offs:**\n",
        "   - Sometimes improvement in one metric might reduce another (e.g., increasing recall might reduce precision). The F1-score helps balance this trade-off.\n",
        "   - Choose the metric(s) that matter most for your business use case.\n",
        "\n",
        "4. **Visualization through Score Charts:**\n",
        "   - Visualize the before-and-after metrics in bar charts or line plots.\n",
        "   - This makes differences clear and helps communicate improvements.\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "Here is a short summary of key evaluation metrics and their business impact:\n",
        "\n",
        "- **Accuracy:** Measures overall correctness. Useful when classes are balanced. High accuracy means fewer wrong decisions, but can be misleading with imbalanced data.\n",
        "\n",
        "- **Precision:** Of all predicted churners, how many actually churned. High precision means less wasted effort on false alarms, saving marketing costs.\n",
        "\n",
        "- **Recall:** Of all actual churners, how many were detected. High recall reduces lost customers by proactive retention.\n",
        "\n",
        "- **F1-Score:** Balance between precision and recall. Important for imbalanced data to optimize both false positives and false negatives.\n",
        "\n",
        "\n",
        "### Business Impact of ML Model:\n",
        "\n",
        "- Improved **recall** helps retain more valuable customers by identifying them early.\n",
        "- Good **precision** reduces unnecessary retention spending.\n",
        "- Balanced **F1-score** leads to efficient resource allocation in churn management.\n",
        "- Overall, the model drives revenue protection and cost efficiency for the business.\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "\n",
        "# Define the model\n",
        "svm_model = SVC(class_weight='balanced', probability=True, random_state=42)\n",
        "\n",
        "# Fit the Algorithm\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "\n",
        "# Generate detailed classification report\n",
        "report_svm = classification_report(y_test, y_pred_svm)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_svm)\n",
        "print(\"Classification Report:\\n\", report_svm)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Calculate metrics for SVM predictions\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred_svm, average=None)\n",
        "accuracy = accuracy_score(y_test, y_pred_svm)\n",
        "\n",
        "# Metrics and scores\n",
        "metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy']\n",
        "class_0_scores = [precision[0], recall[0], f1_score[0], 0]\n",
        "class_1_scores = [precision[1], recall[1], f1_score[1], 0]\n",
        "overall_accuracy = [0, 0, 0, accuracy]\n",
        "\n",
        "# Bar width and positions\n",
        "bar_width = 0.25\n",
        "r1 = np.arange(len(metrics))\n",
        "r2 = [x + bar_width for x in r1]\n",
        "r3 = [x + 2 * bar_width for x in r1]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(r1, class_0_scores, color='blue', width=bar_width, edgecolor='grey', label='Class 0 (No churn)')\n",
        "plt.bar(r2, class_1_scores, color='orange', width=bar_width, edgecolor='grey', label='Class 1 (Churn)')\n",
        "plt.bar(r3, overall_accuracy, color='green', width=bar_width, edgecolor='grey', label='Overall Accuracy')\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel('Evaluation Metrics', fontweight='bold', fontsize=12)\n",
        "plt.ylabel('Score', fontweight='bold', fontsize=12)\n",
        "plt.xticks([r + bar_width for r in range(len(metrics))], metrics)\n",
        "plt.ylim([0, 1])\n",
        "plt.title('Evaluation Metric Scores for SVM Model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Define model and hyperparameter grid for tuning\n",
        "svm = SVC(class_weight='balanced', probability=True, random_state=42)\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "grid_search_svm = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "best_svm_model = grid_search_svm.best_estimator_\n",
        "y_pred_svm_tuned = best_svm_model.predict(X_test)\n",
        "\n",
        "# Optional: print best parameters and classification report\n",
        "print(\"Best Parameters:\", grid_search_svm.best_params_)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svm_tuned))\n"
      ],
      "metadata": {
        "id": "NPJmweDAMUEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best hyperparameters found via GridSearchCV for the SVM model are:\n",
        "\n",
        "- C = 0.1: Regularization strength (lower value means more regularization)\n",
        "- Gamma = 'scale':Kernel coefficient for the RBF kernel, controlling influence of single training examples\n",
        "- Kernel = 'rbf': Radial basis function kernel was selected for non-linear separation\n",
        "\n",
        "***\n",
        "\n",
        "### Evaluation Summary with Best Parameters\n",
        "\n",
        "| Metric    | Class 0 (No churn) | Class 1 (Churn) | Overall Accuracy |\n",
        "|-----------|--------------------|-----------------|------------------|\n",
        "| Precision | 0.86               | 0.47            |                  |\n",
        "| Recall    | 0.73               | 0.67            | 0.71             |\n",
        "| F1-Score  | 0.79               | 0.56            |                  |\n",
        "\n",
        "\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "- The tuned SVM performs similarly to previous runs, showing that this kernel and parameter set balances complexity and generalization well.\n",
        "- The model is good at identifying non-churn customers (precision 0.86) and moderately effective at catching churners (recall 0.67).\n",
        "- Moderate precision on churners indicates some false positives, which may lead to unnecessary retention costs.\n"
      ],
      "metadata": {
        "id": "sAFGMOXaNpnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The hyperparameter optimization technique used is **GridSearchCV** because it exhaustively searches all specified parameter combinations with cross-validation, ensuring reliable model tuning. It’s easy to implement and works well for small to medium search spaces like SVM parameters."
      ],
      "metadata": {
        "id": "0nlF6LbyOXt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "The tuned SVM model shows **similar performance** to the untuned version, with:\n",
        "\n",
        "- Accuracy around **71%**,\n",
        "- Precision and recall for the churn class remain at **47%** and **67%** respectively,\n",
        "- F1-score for churn around **56%**, same as before tuning.\n",
        "\n",
        "\n",
        "\n",
        "### Interpretation of Improvement\n",
        "\n",
        "- No significant improvement is observed in this tuning round.\n",
        "- This suggests the model’s performance is limited by the data or features rather than hyperparameters alone.\n",
        "- You may consider improving feature engineering, trying other algorithms, or more advanced tuning methods.\n",
        "\n",
        "\n",
        "\n",
        "### Updated Evaluation Metric Score Chart (Theoretical)\n",
        "\n",
        "| Metric    | Untuned SVM | Tuned SVM |\n",
        "|-----------|-------------|-----------|\n",
        "| Accuracy  | 0.71        | 0.71      |\n",
        "| Precision | 0.47        | 0.47      |\n",
        "| Recall    | 0.67        | 0.67      |\n",
        "| F1-Score  | 0.56        | 0.56      |\n",
        "\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "For positive business impact, the evaluation metrics considered are:\n",
        "\n",
        "- **Recall (Churn class):** To identify as many actual churners as possible, minimizing lost customers.\n",
        "- **Precision (Churn class):** To reduce false positives, avoiding wasted retention efforts and costs.\n",
        "- **F1-Score:** To balance recall and precision, ensuring both effective and efficient churn targeting.\n",
        "- **Accuracy:** Considered but less prioritized due to class imbalance risks.\n",
        "\n",
        "These metrics align with maximizing customer retention while optimizing resource use."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "The final chosen ML model is the **Random Forest** (Model 2) because:\n",
        "\n",
        "- It balances complexity and interpretability well.\n",
        "- It showed consistent performance with good recall on the churn class, helping catch more churners.\n",
        "- It handles feature interactions and non-linearities better than Logistic Regression and SVM in this dataset.\n",
        "- Hyperparameter tuning improved its robustness without overfitting.\n",
        "\n",
        "Overall, Random Forest offers a strong trade-off between prediction accuracy and practical business usefulness for churn prediction in this case."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The chosen model is Random Forest, an ensemble of decision trees that combines multiple trees’ predictions to improve accuracy and reduce overfitting. It captures non-linear relationships and feature interactions well, making it effective for complex datasets like churn prediction.\n",
        "\n",
        "**Model Explanation**\n",
        "* Random Forest builds many decision trees on random subsets of data/features.\n",
        "\n",
        "* Each tree votes, and the majority vote determines the final prediction.\n",
        "\n",
        "* It naturally handles categorical features and missing values.\n",
        "\n",
        "* Class weighting is applied to balance churn vs no churn."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import joblib\n",
        "\n",
        "# Save your trained Random Forest model\n",
        "joblib.dump(best_rf_model, 'best_rf_model.joblib')\n",
        "\n",
        "print(\"Model saved successfully as best_rf_model.joblib\")\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create or ensure unseen_data.csv exists\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists('unseen_data.csv'):\n",
        "    # If X_train exists, build a realistic one-row sample using X_train medians/modes\n",
        "    if 'X_train' in globals():\n",
        "        cols = list(X_train.columns)\n",
        "        sample = {}\n",
        "        for c in cols:\n",
        "            if pd.api.types.is_numeric_dtype(X_train[c]):\n",
        "                sample[c] = float(X_train[c].median())\n",
        "            else:\n",
        "                # take the most common category; if none exist, use empty string\n",
        "                sample[c] = X_train[c].mode().iloc[0] if not X_train[c].mode().empty else \"\"\n",
        "        unseen_df = pd.DataFrame([sample])\n",
        "    else:\n",
        "        # Fallback: small example with common Telco features (adjust if your model expects different columns)\n",
        "        unseen_df = pd.DataFrame([{\n",
        "            'gender':'Male','SeniorCitizen':0,'Partner':'No','Dependents':'No','tenure':5,\n",
        "            'PhoneService':'Yes','MultipleLines':'No','InternetService':'DSL','OnlineSecurity':'No',\n",
        "            'OnlineBackup':'No','DeviceProtection':'No','TechSupport':'No','StreamingTV':'No',\n",
        "            'StreamingMovies':'No','Contract':'Month-to-month','PaperlessBilling':'Yes',\n",
        "            'PaymentMethod':'Electronic check','MonthlyCharges':70.35,'TotalCharges':350.50\n",
        "        }])\n",
        "    unseen_df.to_csv('unseen_data.csv', index=False)\n",
        "    print(\"Created sample unseen_data.csv with columns:\", list(unseen_df.columns))\n",
        "else:\n",
        "    print(\"unseen_data.csv already exists in the working directory.\")\n"
      ],
      "metadata": {
        "id": "655H7VcYYNw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "loaded_model = joblib.load('best_rf_model.joblib')\n",
        "print(\"Loaded model:\", type(loaded_model))\n",
        "\n",
        "# Load unseen data\n",
        "unseen_df = pd.read_csv('unseen_data.csv')\n",
        "print(\"Unseen data preview:\")\n",
        "display(unseen_df.head())\n",
        "print(\"Unseen shape:\", unseen_df.shape)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict with the loaded model\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('best_rf_model.joblib')\n",
        "\n",
        "# Load unseen data\n",
        "unseen_df = pd.read_csv('unseen_data.csv')\n",
        "print(\"Unseen Data Preview:\")\n",
        "display(unseen_df.head())\n",
        "print(\"Unseen shape:\", unseen_df.shape)\n",
        "\n",
        "# Predict\n",
        "predictions = loaded_model.predict(unseen_df)\n",
        "\n",
        "print(\"\\nPredictions on unseen data:\")\n",
        "print(predictions)\n",
        "\n",
        "# If you want probabilities as well\n",
        "if hasattr(loaded_model, \"predict_proba\"):\n",
        "    probabilities = loaded_model.predict_proba(unseen_df)[:, 1]  # Class 1 probability\n",
        "    print(\"Predicted Probabilities:\", probabilities)\n"
      ],
      "metadata": {
        "id": "os0ViY2VY7n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = {\n",
        "    'InternetService_Fiber optic': [1.0],  # Fiber optic = high churn risk\n",
        "    'Contract_Two year': [0.0],            # Month-to-month = high churn risk\n",
        "    'TotalCharges_19.6': [0.0],\n",
        "    'TotalCharges_19.9': [0.0],\n",
        "    'TotalCharges_20.1': [0.0],\n",
        "    'TotalCharges_20.15': [0.0],\n",
        "    'TotalCharges_20.2': [0.0],\n",
        "    'TotalCharges_20.5': [0.0],\n",
        "    'TotalCharges_20.9': [0.0],\n",
        "    'TotalCharges_259.8': [1.0],\n",
        "    'TotalCharges_288.05': [1.0],\n",
        "    'TotalCharges_45.1': [0.0],\n",
        "    'TotalCharges_45.7': [0.0],\n",
        "    'TotalCharges_50.45': [0.0],\n",
        "    'TotalCharges_740.3': [0.0]\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "unseen_df = pd.DataFrame(sample_data)\n",
        "unseen_df.to_csv('unseen_data.csv', index=False)\n",
        "print(unseen_df)\n"
      ],
      "metadata": {
        "id": "cml3Ms8nZ-Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction again\n",
        "\n",
        "predictions = loaded_model.predict(unseen_df)\n",
        "probabilities = loaded_model.predict_proba(unseen_df)[:, 1]\n",
        "\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"Probabilities:\", probabilities)\n"
      ],
      "metadata": {
        "id": "Z-txGIPhaCLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}